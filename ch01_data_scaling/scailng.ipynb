{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65593554",
   "metadata": {},
   "source": [
    "## 스케일링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4a95e68",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'plotly'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplotly\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexpress\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpx\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'plotly'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796e3e8c",
   "metadata": {},
   "source": [
    "## 샘플 데이터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec8fb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "X = np.random.rand(100,3) # 샘플 100개, 특성 3개\n",
    "# 1번 특성과 차이를 두기\n",
    "X[:,1] *= 12 # 2번 특성은 10배 이상 증가\n",
    "X[:,2] += 100 # 3번 특성은 100을 더해 평균 이동 \n",
    "\n",
    "df = pd.DataFrame(X, columns = ['Feature_A', 'Feature_B', 'Feature_C'])\n",
    "\n",
    "# 분포 확인\n",
    "print('--- 데이터 기초 통계량 ---')\n",
    "display(df.describe())\n",
    "\n",
    "df_melted = df.melt(var_name = 'Feature', value_name = 'Value')\n",
    "\n",
    "fig = px.box(df_melted,\n",
    "             x = 'Value',\n",
    "             y = 'Feature',\n",
    "             color = 'Feature',\n",
    "             title = 'Data Distribution')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3463fd",
   "metadata": {},
   "source": [
    "# 스케일링 수행\n",
    "__표준화 (Standardization) : 평균을 0, 표준편차를 1로 변환__\n",
    "* 정규 분포를 가정하는 알고리즘에 적합 (예 : 선형회귀, 로지스틱 회귀, SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f5f07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1. 데이터 분할 / 전체 데이터를 8 : 2 비율로 나눈다.\n",
    "X_train, X_test = train_test_split(df, test_size = 0.2, random_state = 1234)\n",
    "\n",
    "# 2. 스케일러 정의 및 학습\n",
    "ss = StandardScaler()\n",
    "\n",
    "# fit은 train에만 데이터 누수 방지\n",
    "X_train_ss = ss.fit_transform(X_train)\n",
    "X_test_ss = ss.transform(X_test)\n",
    "\n",
    "# 3. 결과 데이터 프레임 구축\n",
    "df_train_ss = pd.DataFrame(X_train_ss, columns=X_train.columns)\n",
    "df_test_ss = pd.DataFrame(X_test_ss, columns=X_test.columns)\n",
    "\n",
    "print(\"--- [검증] Train 데이터 표준화 통계량 ---\")\n",
    "display(df_train_ss.describe().round(2)) # 정확히 mean=0, std=1\n",
    "\n",
    "print(\"\\n--- [검증] Test 데이터 표준화 통계량 (Train 기준 적용) ---\")\n",
    "display(df_test_ss.describe().round(2)) # 0과 1에 가깝지만 완벽히 일치하진 않음 (정상)\n",
    "\n",
    "# 4. 시각화 \n",
    "fig_ss = px.box(df_train_ss.melt(var_name='Feature', value_name='Value'), \n",
    "                 x=\"Value\", y=\"Feature\", color=\"Feature\", \n",
    "                 title='Standardized Train Data')\n",
    "fig_ss.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a8fad4",
   "metadata": {},
   "source": [
    "__정규화 (Normalization) : 최소값을 0으로 최대값을 1로 변환__\n",
    "* KNN, 이미지 처리, 신경망의 입력측 등에서 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf12a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 2. 스케일러 정의 및 학습\n",
    "mm = MinMaxScaler()\n",
    "\n",
    "# fit은 train에만 데이터 누수 방지\n",
    "X_train_mm = mm.fit_transform(X_train)\n",
    "X_test_mm = mm.transform(X_test)\n",
    "\n",
    "# 3. 결과 데이터 프레임 구축\n",
    "df_train_mm = pd.DataFrame(X_train_mm, columns = X_train.columns)\n",
    "df_test_mm = pd.DataFrame(X_test_mm, columns = X_test.columns)\n",
    "\n",
    "print('--- [검증] Train 데이터 정규화 통계량 ---')\n",
    "display(df_train_mm.describe())\n",
    "print('\\n--- [검증] Test 데이터 정규화 통계량 ---')\n",
    "display(df_test_mm.describe())\n",
    "\n",
    "# 4. 시각화\n",
    "fig_mm = px.box(df_train_mm.melt(var_name = 'Feature', value_name = 'Value'),\n",
    "                x = 'Value',\n",
    "                y = 'Feature',\n",
    "                color = 'Feature',\n",
    "                title = 'Min_Max Scaled Train Data')\n",
    "fig_mm.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f32447",
   "metadata": {},
   "source": [
    "__절대값 기준 정규화 (MaxAbs Scaler) : 절대값 최대치를 1로 스케일링__\n",
    "* 희소(sparse)데이터, 텍스트 데이터의 TF-IDF에 적합, 0을 중심으로 대칭적인 분포 유지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017bd178",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "# 2. 스케일러 정의 및 학습\n",
    "mas = MaxAbsScaler()\n",
    "\n",
    "# fit은 train에만 데이터 누수 방지\n",
    "X_train_mas = mas.fit_transform(X_train)\n",
    "X_test_mas = mas.transform(X_test)\n",
    "\n",
    "# 3. 결과 데이터 프레임 구축\n",
    "df_train_mas = pd.DataFrame(X_train_mas, columns = X_train.columns)\n",
    "df_test_mas = pd.DataFrame(X_test_mas, columns = X_test.columns)\n",
    "\n",
    "print('--- [검증] Train 데이터 MaxAbs 통계량 ---')\n",
    "display(df_train_mas.describe())\n",
    "print('\\n--- [검증] Test 데이터 MaxAbs 통계량 ---')\n",
    "display(df_test_mas.describe())\n",
    "\n",
    "# 4. 시각화\n",
    "fig_mas = px.box(df_train_mas.melt(var_name = 'Feature', value_name = 'Value'),\n",
    "                x = 'Value',\n",
    "                y = 'Feature',\n",
    "                color = 'Feature',\n",
    "                title = 'Max_Abs Scaled Train Data')\n",
    "fig_mas.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2024ff",
   "metadata": {},
   "source": [
    "__로버스트 스케일링 (Robust Scaling) : 중앙값 (median)과 사분위 범위 (IQR)를 사용__\n",
    "* 이상치가 많은 데이터에 적합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2eaac10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# 2. 스케일러 정의 및 학습\n",
    "rs = RobustScaler()\n",
    "\n",
    "# fit은 train에만 데이터 누수 방지\n",
    "X_train_rs = rs.fit_transform(X_train)\n",
    "X_test_rs = rs.transform(X_test)\n",
    "\n",
    "# 3. 결과 데이터 프레임 구축\n",
    "df_train_rs = pd.DataFrame(X_train_rs, columns = X_train.columns)\n",
    "df_test_rs = pd.DataFrame(X_test_rs, columns = X_test.columns)\n",
    "\n",
    "print('--- [검증] Train 데이터 Robust 통계량 ---')\n",
    "display(df_train_rs.describe())\n",
    "print('\\n--- [검증] Test 데이터 Robust 통계량 ---')\n",
    "display(df_test_rs.describe())\n",
    "\n",
    "# 4. 시각화\n",
    "fig_rs = px.box(df_train_rs.melt(var_name = 'Feature', value_name = 'Value'),\n",
    "                x = 'Value',\n",
    "                y = 'Feature',\n",
    "                color = 'Feature',\n",
    "                title = 'Robust Scaled Train Data')\n",
    "fig_rs.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0a40e6",
   "metadata": {},
   "source": [
    "__로그 변환 (Log Transformation) : 데이터 왜곡 수정__\n",
    "* 지수적으로 증가하는 데이터, 한쪽 꼬리가 늘어진 경우(왜도가 높을 때)에 정규분포로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5f9838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 로그 변환 수행 (log1p = log(1+x))\n",
    "X_train_log = np.log1p(X_train)\n",
    "X_test_log = np.log1p(X_test)\n",
    "\n",
    "# 3. 결과 데이터 프레임 구축\n",
    "X_train_log_df = pd.DataFrame(X_train_log, columns = df.columns)\n",
    "\n",
    "print('--- [검증] 로그 변환 후 기초 통계량 ---')\n",
    "display(X_train_log_df.describe())\n",
    "\n",
    "# 4. 시각화\n",
    "fig_log = px.histogram(X_train_log_df.melt(), \n",
    "                       x=\"value\", color=\"variable\", \n",
    "                       facet_col=\"variable\", \n",
    "                       title=\"Distribution After Log Transformation\",\n",
    "                       ) \n",
    "fig_log.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5dc738",
   "metadata": {},
   "source": [
    "__제곱근 변환 (Square Root Transformation) : 데이터에 제곱근을 취함__\n",
    "* 로그 변환보다 덜 극단 적인 변환이 필요한 경우\n",
    "* 포아송 분포를 따르는 데이터(특정 시간동안 발생하는 사건 수)를 정규화 할 때 쓰임\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab302b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 제곱근 변환 수행 \n",
    "X_train_sqrt = np.sqrt(X_train)\n",
    "X_test_sqrt = np.sqrt(X_test)\n",
    "\n",
    "# 3. 결과 데이터 프레임 구축\n",
    "X_train_sqrt_df = pd.DataFrame(X_train_sqrt, columns=df.columns)\n",
    "\n",
    "print(\"--- [검증] 제곱근 변환 후 기초 통계량 ---\")\n",
    "display(X_train_sqrt_df.describe())\n",
    "\n",
    "# 4. 시각화\n",
    "fig_sqrt = px.histogram(X_train_sqrt_df.melt(), \n",
    "                        x=\"value\", color=\"variable\", \n",
    "                        facet_col=\"variable\", \n",
    "                        title=\"Distribution After Square Root Transformation\",\n",
    "                        )\n",
    "fig_sqrt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7c27c4",
   "metadata": {},
   "source": [
    "__박스-콕스 변환 (Box-Cox Transformation) : 데이터를 정규 분포에 가깝게 변환__\n",
    "* 정규성 가정이 중요한 알고리즘에 사용, 양수 데이터만 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dae85e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 박스-콕스 변환 및 람다 정의\n",
    "from scipy import stats\n",
    "X_train_bc = np.zeros_like(X_train)\n",
    "X_test_bc = np.zeros_like(X_test)\n",
    "lambdas = [] \n",
    "\n",
    "# 2. 최적 람다 찾기\n",
    "for i in range(X_train.shape[1]):\n",
    "    X_train_bc[:, i], lmbda = stats.boxcox(X_train.iloc[:, i])\n",
    "    lambdas.append(lmbda)\n",
    "    # 데이터 누수 방지를 위해 반드시 테스트 데이터에는 트레인에서 구한 람다를 쓴다.\n",
    "    X_test_bc[:, i] = stats.boxcox(X_test.iloc[:, i], lmbda=lmbda)\n",
    "\n",
    "# 3. 결과 데이터 프레임 구축\n",
    "# 람다 값 확인 (0에 가까우면 로그, 0.5면 제곱근, 1이면 변환 거의 없음)\n",
    "print(f\"--- 각 특성별 최적화된 Lambda 값: {np.round(lambdas, 3)} ---\")\n",
    "df_train_bc = pd.DataFrame(X_train_bc, columns=df.columns)\n",
    "\n",
    "# 4. 수치적 검증 (기초 통계량)\n",
    "print(\"\\n--- [검증] Box-Cox 변환 후 기초 통계량 (Train 기준) ---\")\n",
    "display(df_train_bc.describe())\n",
    "\n",
    "# 5. 시각화\n",
    "fig_bc = px.histogram(df_train_bc.melt(), \n",
    "                      x=\"value\", \n",
    "                      color=\"variable\", \n",
    "                      facet_col=\"variable\", \n",
    "                      title=\"Box-Cox Transformation: Automating the Search for Normality\",\n",
    "                     ) \n",
    "fig_bc.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5220590",
   "metadata": {},
   "source": [
    "__양자화 (Quantile Transformation) : 데이터를 균일 분포 또는 정규 분포로 변환__\n",
    "* 이상치가 많거나 복잡한 분포를 가진 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f63b6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 양자화\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "qt = QuantileTransformer(output_distribution='normal', random_state = 1234) # output_distribution = normal -> 정규분포\n",
    "\n",
    "# 2. 결과 데이터 프레임 구축\n",
    "X_train_qt = qt.fit_transform(X_train)\n",
    "X_test_qt = qt.transform(X_test)\n",
    "\n",
    "df_train_qt = pd.DataFrame(X_train_qt, columns = df.columns)\n",
    "\n",
    "print('--- [검증] 양자화 변환 후 기초 통계량 ---')\n",
    "display(df_train_qt.describe())\n",
    "\n",
    "# 3. 시각화\n",
    "fig_qt = px.histogram(df_train_qt.melt(),\n",
    "                      x = 'value',\n",
    "                      color = 'variable',\n",
    "                      facet_col = 'variable',\n",
    "                      title = \"Quantile Transformation: Forced Normal Distribution\")\n",
    "fig_qt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d67cb83",
   "metadata": {},
   "source": [
    "__파워 변환 (Power Transformation) : Yeo-Johnson 또는 Box-Cox 변환 사용__\n",
    "* 양수 데이터만 쓰이는 Box-Cox를 보완하여 음수도 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1971bb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 파워변환\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "pt = PowerTransformer(method = 'yeo-johnson', standardize = True)\n",
    "\n",
    "# 2. 결과 데이터 프레임 구축\n",
    "X_train_pt = pt.fit_transform(X_train)\n",
    "X_test_pt = pt.transform(X_test)\n",
    "\n",
    "df_train_pt = pd.DataFrame(X_train_pt, columns = X_train.columns)\n",
    "\n",
    "print(f'--- 각 특성별 최적 람다값 : {np.round(pt.lambdas_,3)} ---')\n",
    "print('\\n--- [검증] 파워 변환 후 기초 통계량 ---')\n",
    "display(df_train_pt.describe())\n",
    "\n",
    "# 3. 시각화\n",
    "fig_pt = px.histogram(df_train_pt.melt(),\n",
    "                      x = 'value',\n",
    "                      color = 'variable',\n",
    "                      facet_col = 'variable',\n",
    "                      title = \"Quantile Transformation: Forced Normal Distribution\")\n",
    "fig_pt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626da4d8",
   "metadata": {},
   "source": [
    "__L1 정규화 (L1 Normalization) : 각 샘플의 L1 norm이 1이 되도록 스케일링__\n",
    "* 텍스트 분류 등에서 사용\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7d5ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. L1 정규화\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "l1 = Normalizer(norm = 'l1')\n",
    "\n",
    "X_train_l1 = l1.fit_transform(X_train)\n",
    "X_test_l1 = l1.transform(X_test)\n",
    "\n",
    "# 2. 결과 데이터 프레임 구축\n",
    "df_train_l1 = pd.DataFrame(X_train_l1, columns = X_train.columns)\n",
    "\n",
    "print('--- [검증] L1 정규화 후 첫 번째 행의 합 ---')\n",
    "print(df_train_l1.iloc[0].abs().sum())\n",
    "\n",
    "display(df_train_l1.head().round(4))\n",
    "\n",
    "# 3. 시각화\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "# 타격 지점 1: 각 특성의 분포 변화 (KDE Plot)\n",
    "plt.subplot(1, 3, 1)\n",
    "for col in df_train_l1.columns:\n",
    "    sns.kdeplot(df_train_l1[col], label=col, fill=True)\n",
    "plt.title(\"Feature Distributions (L1 Normalized)\")\n",
    "plt.xlabel(\"Normalized Value (0 to 1)\")\n",
    "plt.legend()\n",
    "\n",
    "# 타격 지점 2: 개별 샘플의 특성 비중 (Stacked Bar - 첫 10개 행)\n",
    "# [Rationale] L1의 핵심은 행 내 합이 1이라는 점을 시각적으로 보여주는 것\n",
    "plt.subplot(1, 3, 2)\n",
    "df_train_l1.head(10).plot(kind='bar', stacked=True, ax=plt.gca())\n",
    "plt.title(\"Sample-wise Feature Proportions (Top 10)\")\n",
    "plt.xlabel(\"Sample Index\")\n",
    "plt.ylabel(\"Proportion (Sum = 1)\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# 타격 지점 3: 정규화 전/후의 상관관계 유지 확인 (Heatmap)\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.heatmap(df_train_l1.corr(), annot=True, cmap='RdBu_r', center=0)\n",
    "plt.title(\"Feature Correlation Matrix\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAK-DS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
